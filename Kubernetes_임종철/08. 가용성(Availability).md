# 08. 가용성(Availability)

### 샘플 어플리케이션 다운로드
```zsh
$ git clone https://github.com/ToruMakabe/Understanding-K8s
$ cd Understanding-K8s/chap08
```
* 멀티 클러스터 환경을 구축하는 Terraform 샘플코드

---
## 8.1 쿠버네티스의 가용성

---
#### 1. 마스터의 가용성(모두 액티브한 etcd와 API Server)
* 컨트롤 플레인을 구성하는 '쿠너네티스 컴포넌트'는 주로 '마스터 서버' 상에 배치됨
	- 그리고 로드밸런서를 사용하여 액세스를 분산시킴

![마스터 컨포넌트 관계](./08. 가용성(Availability)/kubernetes_00_마스터컨포넌트의관계.jpg)[^출처]
* etcd의 가용성을 높이는 방법
	- etcd 바이너리의 증명서, 각종 파일을 대상 서버에 배포하고 가동
	- 각 서버에 있는 etcd가 '맴버'로서 etcd 클러스터에 참가
* '마스터 서버'를 다중화하여 각각에서 etcd를 움직이는 것	

##### etcd versus other key-value stores
* 참고 : https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md

* API Server의 배치
	- 다중화한 모든 '마스터 서버'에 배치하고 로드밸런서로 액세스 분산

---
#### 2. 마스터의 가용성(액티브/스탠바이 컴포넌트)
* 컨트롤러 매니저(Controller Manager)와 스케줄러(Scheduler)는 스탠바이 상태로 장애에 대비
	- 상태를 유지하는 루프 작동시, 동시에 작동되어 _경합_을 벌일 우러가 있음

![컨트롤러매니저와 스케줄러 리더 구조](./08. 가용성(Availability)/kubernetes_01_컨트롤러매니저와스케줄러리더구조.jpg)[^출처]
* 가장 빨리 쓴 것이 리더가 되어 정기적으로 갱신 시각을 기록(기본값: 2초 간격)
* 이 정보를 'Endpoinr 오브젝트'를 사용하고 있지만, 컨피그맵으로 대체될 예정
	- 엔드포인트 'kube=controller-manager'에 리더명과 갱신 시각이 기록
	- 갱신이 멈춘 경우, 리더가 되려고 엔드포인트 오브젝트에 자신의 이름 쓰기를 반복 시도

Ex) 엔드포인트 'kube-controller-manager' 확인 
```zsh
[] ~/Understanding-K8s/chap08 <master> ✗ kubectl get endpoints kube-controller-manager -n kube-system -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"kube-controller-manager-6dc949ff64-kvxkp_18ff4610-6c93-4fcc-8ef8-9d9699ae632a","leaseDurationSeconds":15,"acquireTime":"2020-03-26T05:39:58Z","renewTime":"2020-03-26T05:47:36Z","leaderTransitions":0}'
  creationTimestamp: "2020-03-26T05:39:58Z"
  name: kube-controller-manager
  namespace: kube-system
  resourceVersion: "1327"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager
  uid: 2d1f2156-efdb-4ede-82f8-3a21a35a1dfb
```
* 리더 선출에는 [metadata]-[annotations]의 [control-plane.alpha.kubernetes.io/leader:]를 사용
	- 현재 "holderIdentity"가 리더
	- "acquireTime":"2020-03-26T05:39:58Z" : 리더가 된 시간
	- "renewTime":"2020-03-26T05:47:36Z" : 갱신된 시간


---
#### 3. 노드의 가용성
* 마스터 서버(etcd와 API Server)와 마찬가지로 동일한 구성으로 된 서버를 늘리는 스케일 아웃이 기본적인 방식

---
#### 4. 분산 수를 어떻게 할까? (마스터)
* 마스터 노드수는 홀수를 권장
	- 일반적으로 3, 최대 7대 정도(너무 많으면 오버헤드가 커짐)
* 홀수로 하는 이유는 데이터스토어인 etcd의 무결성을 위해함
	- 데이너 부정합이 일어나지 않도록 분산 데이터스토어(etcd)의 맴버는 네트워크를 통해 항상 합의하에 데이터 갱신함
	- 네트워크 분단이 일어나면 '다수결'에 의해 그룹 한쪽만 __갱신__, 분단 해소된 후 적은 그룹이 덮어쓰기 됨
![etcd에서 네트워크분단이 일어나면](./08. 가용성(Availability)/kubernetes_02_etcd에서네트워크분단이일어나면.jpg)[^출처]

* etcd는 Raft라는 분산 합의 알고리즘을 사용
	- 액티브한 맴버 중에서 리더를 선출, 그 리더를 중심으로 데이터를 갱신
##### The Raft Consensus Algorithm
* 참고 : https://raft.github.io/

---
#### 5. 분산 수를 어떻게 할까? (노드)
* 다중화가 필요하면 서버 2대 이상에서 필요한 리소스의 양을 보고 결정
* 노드는 3개 이상으로 구성하는 것을 권장
	- 서버 다운 시의 영향을 고려(서버가 2대의 경우, 서버 다운 시 남은 1대의 부하량 및 리소스의 양이 너무 많음)

---
## 8.2 인프라스트럭처의 시점

---
#### 1. Blast Radius(폭발 반경)
* 가용성을 검토할 때는 항상 장애의 영향 범위를 의식하여 요건에 따라 분산시켜야 함
* Blast Radius
	- 장애긔 영향이 미치는 범위
	- Blast : 고장, 재해(시스템의 세계에서)

##### 분산 배치 레벨
1. 물리 서버(가상머신의 경우)
	* 가상 머신을 동일한 물리 서버에 배치하면, 물리 서버에 장애가 발생시 모두 영향을 받음
		- 최소한 다른 물리 서버에 배치 필요	
2. 랙
	* 일반적으로 물리 서버에 대한 전력, 네트워크는 랙 단위로 묶음
	* 서버의 네트워크가 물려있는 ToR(Top of Rack) 스위치가 고장을 일으키면 해당 랙 전체 서버에 영향을 미침
		- 명시적으로 랙을 나뉘 가상 머신을 분산 배치 필요
	* 랙의 네트워크, 전원 계통의 이중화 권장
3. 데이터 센터
	* 배전, 발전 장치, 공조 등 건물 전테에 영향을 줄 수 있는 공유 설비가 존재
		- 일반적으로는 다중화되어 있지만, 데이터센터를 나눠서 가상 머신을 배치 권장
4. 지역
	* 지진, 홍수 등 여러 데이터 센터가 영향을 받는 광역 재해
		- 한국은 수도권과 지방으로 분산 배치하는 경우가 많음
![Blast Radius](./08. 가용성(Availability)/kubernetes_03_BlastRadius.jpg)[^출처]
* Blast Radius를 의식하여 배치시키는게 가용성을 올리는 포인트

---
#### 2. 소프트웨어적인 Blast Radius
* 클러스터 전체에 영향을 끼칠수 있음
	- 완화를 위한 '클러스터 다중화' 및 회피를 위한 '복수 클러스터 구현'을 생각할 수 있음

---
#### 3. 배치 예
* 케이스 스터디
	- 마스터 가상 머신 : 3대
	- 노드 가상 머신 : 6대

---
#### 4. 물리 서버를 의식한 배치
![물리서버 고장에 견디는구성](./08. 가용성(Availability)/kubernetes_04_물리서버고장에견디는구성.jpg)[^출처]
* 노드 가상 머신은 물리 서버의 수에 따라 균등하게 배치

---
#### 5. 랙을 의식한 배치
![랙 전체의 장애에 견디는구성](./08. 가용성(Availability)/kubernetes_05_랙전체의장애에견디는구성.jpg)[^출처]
* 마스터 가상 머신과 노드를 랙하나에 치우치지 않도록 배치

---
#### 6. 데이터 센터를 의식한 배치
![데이터센터 장에 견디는구성](./08. 가용성(Availability)/kubernetes_06_데이터센터장에견디는구성.jpg)[^출처]
* 세 군데의 데이터 센터에 균등하게 배치
	- 데이터 센터 간의 지연 고려
* etcd의 기본값을 참고해 허용 지연의 기준을 정함
	- 하트비트 : 100ms
	- 리더 선출 타임 아웃: 1000ms
	- 기본 타임 아웃 : RTT(Round Trip Time)의 10배
* etcd에 있어 권장 지연 목표 : 10ms 이내(2~3ms 이내 설계 권장)

##### etcd 튜닝
* 참고 : https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md

---
#### 7. 광격 재해를 의식한 배치
![광역장에 견디는구성](./08. 가용성(Availability)/kubernetes_07_광역장에견디는구성.jpg)[^출처]
* 지연을 대폭 개선하기에는 어려움
	- 지역별로 클러스터를 마련하고 재해 시에 트래픽을 전환하는 구성을 권장

* 트래픽 제어
	- 어플리케이션의 클라이언트로부터 오는 트래픽을 어느쪽 지역으로 보낼지 제어
	- Ex) DNS를 베이스로 한 광역 분하분산 또는 액티브/스탠바이 구성
* 구성 관리
	- 인프라를 포함한 클러스터, 쿠버네티스 오브젝트 구성을 코드로 관리하고 자동으로 디플로이할 수 있는 장치가 필요
* 어플리케이션 데이터 동기
	- 데이터스토어(etcd)를 쿠버네티스 클러스터에 놓지 않고 클러스터 외부의 것을 사용
		+ Ex) 클라우드 서비스가 제공하는 '매니지드 데이터베이스 서비스'
		+ 가용성 및 유지보수에 이익
		
---
#### 8. AKS의 구축 예
* AKS는 마스터 컴포넌트를 '매니지드 서비스'로 제공하고 있음([AKS 리소스 그룹])
	- 마스터 서버의 가상 머신은 보이지 않음
	- API 엔트포인트와 로그 등 '관리용 인터페이스'만 사용자에게 공개

![AKS 리소스 루프와 구성 요소](./08. 가용성(Availability)/kubernetes_08_AKS리소스루프와구성요소.jpg)[^출처]

* [사용자/노드 리소스 그룹] 사용자에게 내용이 보임, 다양한 지정 가능
* Azure는 '가용성 세트'라는 장치를 이용하여 노드를 가상 머신에 배치
	- 랙 레벨의 장애가 발생해도 동일한 역할의 가상 머신이 전멸하지 않음
* AKS의 마스터(쿠버네티스 서비스)는 지역(리전)을 걸치지 않음
	- 다른 서비스나 장치를 조합하여 장애 대응 필요

![AKS의 멀티 리전 /멀티 클러스터 구성](./08. 가용성(Availability)/kubernetes_09_AKS의멀티리전멀티클러스터구성.jpg)[^출처]
* Traffic Manager
	- Azure의 DNS 장치를 활용한 트래픽 제어 서비스
	- 클라이언트에게 공개하는 DNS명과 실제로 트래픽을 받는 엔트 포이트를 조합
	- 여러 설정이 있으나, 액티브/스탠바이를 구현하려면 '우선순위' 라우팅을 사용
* Comos DB
	- 멀티 리전에서 복제 가능한 데이터스토어
	- 쿠버네티스 클러스터에 의존하는 않는 데이터스토어 환경을 실현 가능

* Terraform
	- 멀티 클라우드, 레이어의 다른 기술을 커버 가능 툴
	- '샘플(chap08)'을 통해 환경구축 및 클러스터 간 데이터 동기, 액티브 클러스터의 전환 등 시험 가능
	
##### Open Service Broker API	
* 참고 : https://www.openservicebrokerapi.org/
	

---
## 8.3 정리
* 쿠버네티스 컴포넌트는 여러 개 움직일 수가 있으며, 여러 대의 서버로 분산함으로써 가용성을 향상 가능
* API Server와 etcd는 모두 액티브로 만들 수 있음
* Controller Manager와 Scheduler의 액티브 수는 1개(대)
* Blast Radius를 의식하여 분산 배치 구현
* 쿠버네티스만 고집하지 말고 다른 장치도 조합 권장


---
[^출처]: 완벽한 IT 인프라 구축의 자동화를 위한 Kubernetes-정보문화사